import palimpzest as pz
import palimpzest.operators as ops
from .plan import LogicalPlan, PhysicalPlan
from palimpzest.operators import PhysicalOp, QueryStrategy, Model

from itertools import permutations
from typing import Any, Dict, List, Optional

import os


class Planner:
    """
    A Planner is responsible for generating a set of possible plans.
    The fundamental abstraction is, given an input of a graph (of datasets, or logical operators), it generates a set of possible graphs which correspond to the plan.
    These plans can be consumed from the planner using the __iter__ method.
    """

    def __init__(self):
        self.plans = []

    def generate_plans(self):
        return NotImplementedError

    def __iter__(self):
        return iter(self.plans)

    def __next__(self):
        return next(iter(self.plans))

    def __len__(self):
        return len(self.plans)


class LogicalPlanner(Planner):
    def __init__(self, no_cache: bool = False, *args, **kwargs):
        """A given planner should not have a dataset when it's being generated, since it could be used for multiple datasets.
        However, we currently cannot support this since the plans are stored within a single planner object.
        To support this, we can use a dictionary in the form [dataset -> [Plan, Plan, ...]].
        To discuss for future versions.
        """

        super().__init__(*args, **kwargs)
        self.no_cache = no_cache

    @staticmethod
    def _compute_legal_permutations(
        filterAndConvertOps: List[ops.LogicalOperator],
    ) -> List[List[ops.LogicalOperator]]:
        # There are a few rules surrounding which permutation(s) of logical operators are legal:
        # 1. if a filter depends on a field in a convert's outputSchema, it must be executed after the convert
        # 2. if a convert depends on another operation's outputSchema, it must be executed after that operation
        # 3. if depends_on is not specified for a convert operator, it cannot be swapped with another convert
        # 4. if depends_on is not specified for a filter, it can not be swapped with a convert (but it can be swapped w/adjacent filters)

        # compute implicit depends_on relationships, keep in mind that operations closer to the start of the list are executed first;
        # if depends_on is not specified for a convert or filter, it implicitly depends_on all preceding converts
        for idx, op in enumerate(filterAndConvertOps):
            if op.depends_on is None:
                all_prior_generated_fields = []
                for upstreamOp in filterAndConvertOps[:idx]:
                    if isinstance(upstreamOp, ops.ConvertScan):
                        all_prior_generated_fields.extend(upstreamOp.generated_fields)
                op.depends_on = all_prior_generated_fields

        # compute all permutations of operators
        opPermutations = permutations(filterAndConvertOps)

        # iterate over permutations and determine if they are legal;
        # keep in mind that operations closer to the start of the list are executed first TODO
        legalOpPermutations = []
        for opPermutation in opPermutations:
            is_valid = True
            for idx, op in enumerate(opPermutation):
                # if this op is a filter, we can skip because no upstream ops will conflict with this
                if isinstance(op, ops.FilteredScan):
                    continue

                # invalid if upstream op depends on field generated by this op
                for upstreamOp in opPermutation[:idx]:
                    for col in upstreamOp.depends_on:
                        if col in op.generated_fields:
                            is_valid = False
                            break
                    if is_valid is False:
                        break
                if is_valid is False:
                    break

            # if permutation order is valid, then add it to the list of legal permutations
            if is_valid:
                legalOpPermutations.append(opPermutation)

        return legalOpPermutations

    @staticmethod
    def _compute_logical_plan_reorderings(
        logicalPlan: LogicalPlan,
    ) -> List[LogicalPlan]:
        """
        Given the naive logical plan, compute all possible equivalent plans with filter
        and convert operations re-ordered.
        """
        operators = logicalPlan.operators

        plans, op_idx = [], 0
        while op_idx < len(operators):
            op = operators[op_idx]

            # base case, if this operator is a BaseScan or CacheScan set plans to be the
            # logical plan with just this operator
            if isinstance(op, pz.operators.BaseScan) or isinstance(
                op, pz.operators.CacheScan
            ):
                plans = [LogicalPlan(operators=[op])]
                op_idx += 1

            # if this operator is not a FilteredScan or a ConvertScan: join op with each of the
            # re-orderings for its source operations
            elif not isinstance(op, pz.operators.FilteredScan) and not isinstance(
                op, pz.operators.ConvertScan
            ):
                all_plans = []
                for subplan in plans:
                    new_logical_plan = LogicalPlan.fromOpsAndSubPlan([op], subplan)
                    all_plans.append(new_logical_plan)

                # update plans and op_idx
                plans = all_plans
                op_idx += 1

            # otherwise, if this operator is a FilteredScan or ConvertScan, make one plan per (legal)
            # permutation of consecutive converts and filters and recurse
            else:
                # get list of consecutive converts and filters
                filterAndConvertOps = []
                nextOp, next_idx = op, op_idx
                while isinstance(nextOp, pz.operators.FilteredScan) or isinstance(
                    nextOp, pz.operators.ConvertScan
                ):
                    filterAndConvertOps.append(nextOp)
                    nextOp = (
                        operators[next_idx + 1]
                        if next_idx + 1 < len(operators)
                        else None
                    )
                    next_idx = next_idx + 1

                # compute set of legal permutations
                op_permutations = LogicalPlanner._compute_legal_permutations(
                    filterAndConvertOps
                )

                # compute cross-product of op_permutations and subTrees by linking final op w/first op in subTree
                all_plans = []
                for ops in op_permutations:
                    for subplan in plans:
                        new_logical_plan = LogicalPlan.fromOpsAndSubPlan(ops, subplan)
                        all_plans.append(new_logical_plan)

                # update plans and operators (so that we skip over all operators in the re-ordering)
                plans = all_plans
                op_idx = next_idx

        return plans

    def _construct_logical_plan(self, dataset_nodes: List[pz.Set]) -> LogicalPlan:
        operators = []
        for idx, node in enumerate(dataset_nodes):
            uid = node.universalIdentifier()

            # Use cache if allowed
            if not self.no_cache and pz.datamanager.DataDirectory().hasCachedAnswer(
                uid
            ):
                op = pz.operators.CacheScan(node.schema, datasetIdentifier=uid)
                operators.append(op)
                # return LogicalPlan(operators=operators)
                continue

            # First node is DataSource
            if idx == 0:
                assert isinstance(node, pz.datasources.DataSource)
                op = pz.operators.BaseScan(
                    outputSchema=node.schema,
                    datasetIdentifier=uid,
                )

            # if the Set's source is another Set, apply the appropriate scan to the Set
            else:
                inputSchema = dataset_nodes[idx - 1].schema
                outputSchema = node.schema

                if node._filter is not None:
                    op = pz.operators.FilteredScan(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        filter=node._filter,
                        depends_on=node._depends_on,
                        targetCacheId=uid,
                    )
                elif node._groupBy is not None:
                    op = pz.operators.GroupByAggregate(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        gbySig=node._groupBy,
                        targetCacheId=uid,
                    )
                elif node._aggFunc is not None:
                    op = pz.operators.ApplyAggregateFunction(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        aggregationFunction=node._aggFunc,
                        targetCacheId=uid,
                    )
                elif node._limit is not None:
                    op = pz.operators.LimitScan(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        limit=node._limit,
                        targetCacheId=uid,
                    )
                elif node._fnid is not None:
                    op = pz.operators.ApplyUserFunction(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        fnid=node._fnid,
                        targetCacheId=uid,
                    )
                elif not outputSchema == inputSchema:
                    op = pz.operators.ConvertScan(
                        outputSchema=outputSchema,
                        inputSchema=inputSchema,
                        cardinality=node._cardinality,
                        image_conversion=node._image_conversion,
                        depends_on=node._depends_on,
                        targetCacheId=uid,
                    )
                else:
                    raise NotImplementedError(
                        "No logical operator exists for the specified dataset construction."
                    )

            operators.append(op)

        return LogicalPlan(operators=operators)

    def generate_plans(
        self, dataset: pz.Dataset, sentinels: bool = False
    ) -> List[LogicalPlan]:
        """Return a set of possible logical trees of operators on Sets."""
        # Obtain ordered list of datasets
        dataset_nodes = []
        node = dataset

        while isinstance(node, pz.sets.Dataset):
            dataset_nodes.append(node)
            node = node._source
        dataset_nodes.append(node)
        dataset_nodes = list(reversed(dataset_nodes))

        # construct naive logical plan
        plan = self._construct_logical_plan(dataset_nodes)

        # at the moment, we only consider sentinels for the naive logical plan
        if sentinels:
            self.plans = [plan]
            return self.plans

        # compute all possible logical re-orderings of this plan
        self.plans = LogicalPlanner._compute_logical_plan_reorderings(plan)
        print(f"LOGICAL PLANS: {len(self.plans)}")

        return self.plans


class PhysicalPlanner(Planner):
    def __init__(
        self,
        num_samples: Optional[int] = 10,
        scan_start_idx: Optional[int] = 0,
        sample_execution_data: Optional[List[Dict[str, Any]]] = None,
        allow_model_selection: Optional[bool] = True,
        allow_code_synth: Optional[bool] = True,
        allow_token_reduction: Optional[bool] = True,
        shouldProfile: Optional[bool] = True,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.num_samples = num_samples
        self.scan_start_idx = scan_start_idx
        self.sample_execution_data = sample_execution_data
        self.allow_model_selection = allow_model_selection
        self.allow_code_synth = allow_code_synth
        self.allow_token_reduction = allow_token_reduction
        self.shouldProfile = shouldProfile

        self.physical_ops = pz.operators.PHYSICAL_OPERATORS

    def _getModels(self, include_vision: bool = False):
        models = []
        if os.getenv("OPENAI_API_KEY") is not None:
            models.extend([pz.Model.GPT_3_5, pz.Model.GPT_4])

        if os.getenv("TOGETHER_API_KEY") is not None:
            models.extend([pz.Model.MIXTRAL])

        if os.getenv("GOOGLE_API_KEY") is not None:
            models.extend([pz.Model.GEMINI_1])

        if include_vision:
            models.append(pz.Model.GPT_4V)

        return models

    def compute_operator_estimates(self):
        """
        Compute per-operator estimates of runtime, cost, and quality.
        """
        # compute estimates for every operator
        op_filters_to_estimates = {}
        if self.sample_execution_data is not None and self.sample_execution_data != []:
            # construct full dataset of samples
            df = pd.DataFrame(self.sample_execution_data)

            # get unique set of operator filters:
            # - for base/cache scans this is very simple
            # - for filters, this is based on the unique filter string or function (per-model)
            # - for induce, this is based on the generated field(s) (per-model)
            op_filters_to_estimates = {}
            logical_op = logicalPlans[0]
            while logical_op is not None:
                op_filter, estimates = None, None
                if isinstance(logical_op, BaseScan):
                    op_filter = "op_name == 'base_scan'"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, CacheScan):
                    op_filter = "op_name == 'cache_scan'"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, ConvertScan):
                    generated_fields_str = "-".join(sorted(logical_op.generated_fields))
                    op_filter = f"(generated_fields == '{generated_fields_str}') & (op_name == 'induce' | op_name == 'p_induce')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        # compute estimates per-model, and add None which forces computation of avg. across all models
                        models = self._getModels(include_vision=True) + [None]
                        estimates = {model: None for model in models}
                        for model in models:
                            model_name = model.value if model is not None else None
                            est_tokens = StatsProcessor._est_num_input_output_tokens(
                                op_df, model_name=model_name
                            )
                            model_estimates = {
                                "time_per_record": StatsProcessor._est_time_per_record(
                                    op_df, model_name=model_name
                                ),
                                "cost_per_record": StatsProcessor._est_usd_per_record(
                                    op_df, model_name=model_name
                                ),
                                "est_num_input_tokens": est_tokens[0],
                                "est_num_output_tokens": est_tokens[1],
                                "selectivity": StatsProcessor._est_selectivity(
                                    df, op_df, model_name=model_name
                                ),
                                "quality": StatsProcessor._est_quality(
                                    op_df, model_name=model_name
                                ),
                            }
                            estimates[model_name] = model_estimates
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, FilteredScan):
                    filter_str = (
                        logical_op.filter.filterCondition
                        if logical_op.filter.filterCondition is not None
                        else str(logical_op.filter.filterFn)
                    )
                    op_filter = f"(filter == '{str(filter_str)}') & (op_name == 'filter' | op_name == 'p_filter')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        models = (
                            self._getModels()
                            if logical_op.filter.filterCondition is not None
                            else [None]
                        )
                        estimates = {model: None for model in models}
                        for model in models:
                            model_name = model.value if model is not None else None
                            est_tokens = StatsProcessor._est_num_input_output_tokens(
                                op_df, model_name=model_name
                            )
                            model_estimates = {
                                "time_per_record": StatsProcessor._est_time_per_record(
                                    op_df, model_name=model_name
                                ),
                                "cost_per_record": StatsProcessor._est_usd_per_record(
                                    op_df, model_name=model_name
                                ),
                                "est_num_input_tokens": est_tokens[0],
                                "est_num_output_tokens": est_tokens[1],
                                "selectivity": StatsProcessor._est_selectivity(
                                    df, op_df, model_name=model_name
                                ),
                                "quality": StatsProcessor._est_quality(
                                    op_df, model_name=model_name
                                ),
                            }
                            estimates[model_name] = model_estimates
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, LimitScan):
                    op_filter = "(op_name == 'limit')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif (
                    isinstance(logical_op, ApplyAggregateFunction)
                    and logical_op.aggregationFunction.funcDesc == "COUNT"
                ):
                    op_filter = "(op_name == 'count')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif (
                    isinstance(logical_op, ApplyAggregateFunction)
                    and logical_op.aggregationFunction.funcDesc == "AVERAGE"
                ):
                    op_filter = "(op_name == 'average')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                logical_op = logical_op.inputOp

        return op_filters_to_estimates

    def estimate_plan_costs(self, physical_plans: List[PhysicalPlan], operator_estimates: Any):
        """Estimate the cost (in terms of USD, latency, throughput, etc.) for each plan."""
        plans = []
        sample_execution_data = (
            None if operator_estimates == {} else operator_estimates
        )
        for physical_plan in physical_plans:
            planCost, fullPlanCostEst = physical_plan.estimateCost(
                sample_execution_data=sample_execution_data
            )

            totalTime = planCost["totalTime"]
            totalCost = planCost["totalUSD"]  # for now, cost == USD
            quality = planCost["quality"]

            plans.append((totalTime, totalCost, quality, physical_plan, fullPlanCostEst))
        
        return plans

    def _createBaselinePlan(self, model: pz.Model):
        """A simple wrapper around _createSentinelPlan as right now these are one and the same."""
        return self._createSentinelPlan(model)

    def _createSentinelPlan(self, model: pz.Model):
        """
        Create the sentinel plans, which -- at least for now --- are single model plans
        which follow the structure of the user-specified program.
        """
        # base case: this is a root op
        if self.inputOp is None:
            return self._getPhysicalTree(
                strategy=pz.operators.PhysicalOp.LOCAL_PLAN, shouldProfile=True
            )

        # recursive case: get list of possible input physical plans
        subTreePhysicalPlan = self.inputOp._createSentinelPlan(model)
        subTreePhysicalPlan = subTreePhysicalPlan.copy()

        physicalPlan = None
        if isinstance(self, ConvertScan):
            physicalPlan = self._getPhysicalTree(
                strategy=PhysicalOp.LOCAL_PLAN,
                source=subTreePhysicalPlan,
                model=model,
                query_strategy=QueryStrategy.BONDED_WITH_FALLBACK,
                token_budget=1.0,
                shouldProfile=True,
            )

        elif isinstance(self, FilteredScan):
            physicalPlan = self._getPhysicalTree(
                strategy=PhysicalOp.LOCAL_PLAN,
                source=subTreePhysicalPlan,
                model=model,
                shouldProfile=True,
            )

        else:
            physicalPlan = self._getPhysicalTree(
                strategy=PhysicalOp.LOCAL_PLAN,
                source=subTreePhysicalPlan,
                shouldProfile=True,
            )

        return physicalPlan

    def _createPhysicalPlans(
        self,
        logical_plan: LogicalPlan,
    ) -> List[PhysicalOp]:
        """
        Given the logical plan implied by this LogicalOperator, enumerate up to `max`
        possible physical plans and return them as a list.
        """
        # TODO: for each FilteredScan & ConvertScan try:
        # 1. swapping different models
        #    a. different model hyperparams?
        # 2. different prompt strategies
        #    a. Zero-Shot vs. Few-Shot vs. COT vs. DSPy
        # 3. input sub-selection
        #    a. vector DB, LLM attention, ask-the-LLM

        # choose set of acceptable models based on possible llmservices
        models = self._getModels()
        assert (
            len(models) > 0
        ), "No models available to create physical plans! You must set at least one of the following environment variables: [OPENAI_API_KEY, TOGETHER_API_KEY, GOOGLE_API_KEY]"

        # determine which query strategies may be used
        query_strategies = [QueryStrategy.BONDED_WITH_FALLBACK]
        if self.allow_code_synth:
            query_strategies.append(QueryStrategy.CODE_GEN_WITH_FALLBACK)

        token_budgets = [1.0]
        if self.allow_token_reduction:
            token_budgets.extend([0.1, 0.5, 0.9])

        # base case: this is a root op
        if self.inputOp is None:
            # NOTE: right now, the root op must be a CacheScan or BaseScan which does not require an LLM;
            #       if this ever changes we may need to return a list of physical ops here
            return [
                self._getPhysicalTree(
                    strategy=PhysicalOp.LOCAL_PLAN, shouldProfile=self.shouldProfile
                )
            ]

        # recursive case: get list of possible input physical plans
        subTreePhysicalPlans = self._createPhysicalPlans(self.inputOp)

        def get_models_from_subtree(phys_op):
            phys_op_models = []
            while phys_op is not None:
                phys_op_models.append(getattr(phys_op, "model", None))
                phys_op = phys_op.source

            return phys_op_models

        # compute (list of) physical plans for this op
        physicalPlans = []
        if isinstance(self, ConvertScan):
            for subTreePhysicalPlan in subTreePhysicalPlans:
                for qs in query_strategies:
                    if qs not in [
                        QueryStrategy.CODE_GEN_WITH_FALLBACK,
                        QueryStrategy.CODE_GEN,
                    ]:
                        for token_budget in token_budgets:
                            for model in models:
                                # if model selection is disallowed; skip any plans which would use a different model in this operator
                                subtree_models = [
                                    m
                                    for m in get_models_from_subtree(
                                        subTreePhysicalPlan
                                    )
                                    if m is not None and m != Model.GPT_4V
                                ]
                                if (
                                    not self.allow_model_selection
                                    and len(subtree_models) > 0
                                    and subtree_models[0] != model
                                ):
                                    continue

                                # NOTE: failing to make a copy will lead to duplicate profile information being captured
                                # create a copy of subTreePhysicalPlan and use it as source for this physicalPlan
                                subTreePhysicalPlan = subTreePhysicalPlan.copy()
                                physicalPlan = self._getPhysicalTree(
                                    strategy=PhysicalOp.LOCAL_PLAN,
                                    source=subTreePhysicalPlan,
                                    model=model,
                                    query_strategy=qs,
                                    token_budget=token_budget,
                                    shouldProfile=self.shouldProfile,
                                )
                                physicalPlans.append(physicalPlan)
                                # GV Checking if there is an hardcoded function exposes that we need to refactor the solver/physical function generation
                                td = physicalPlan._makeTaskDescriptor()
                                if td.model == None:
                                    break
                    else:
                        # NOTE: failing to make a copy will lead to duplicate profile information being captured
                        # create a copy of subTreePhysicalPlan and use it as source for this physicalPlan
                        subTreePhysicalPlan = subTreePhysicalPlan.copy()
                        physicalPlan = self._getPhysicalTree(
                            strategy=PhysicalOp.LOCAL_PLAN,
                            source=subTreePhysicalPlan,
                            model=Model.GPT_4,
                            query_strategy=qs,
                            token_budget=1.0,
                            shouldProfile=self.shouldProfile,
                        )
                        physicalPlans.append(physicalPlan)

        elif isinstance(self, FilteredScan):
            for subTreePhysicalPlan in subTreePhysicalPlans:
                for model in models:
                    # if model selection is disallowed; skip any plans which would use a different model in this operator
                    subtree_models = [
                        m
                        for m in get_models_from_subtree(subTreePhysicalPlan)
                        if m is not None and m != Model.GPT_4V
                    ]
                    if (
                        not self.allow_model_selection
                        and len(subtree_models) > 0
                        and subtree_models[0] != model
                    ):
                        continue

                    # NOTE: failing to make a copy will lead to duplicate profile information being captured
                    # create a copy of subTreePhysicalPlan and use it as source for this physicalPlan
                    subTreePhysicalPlan = subTreePhysicalPlan.copy()
                    physicalPlan = self._getPhysicalTree(
                        strategy=PhysicalOp.LOCAL_PLAN,
                        source=subTreePhysicalPlan,
                        model=model,
                        shouldProfile=self.shouldProfile,
                    )
                    physicalPlans.append(physicalPlan)
                    # GV Checking if there is an hardcoded function exposes that we need to refactor the solver/physical function generation
                    td = physicalPlan._makeTaskDescriptor()
                    if td.model == None:
                        break

        else:
            for subTreePhysicalPlan in subTreePhysicalPlans:
                # NOTE: failing to make a copy will lead to duplicate profile information being captured
                # create a copy of subTreePhysicalPlan and use it as source for this physicalPlan
                subTreePhysicalPlan = subTreePhysicalPlan.copy()
                physicalPlan = self._getPhysicalTree(
                    strategy=PhysicalOp.LOCAL_PLAN,
                    source=subTreePhysicalPlan,
                    shouldProfile=self.shouldProfile,
                )
                physicalPlans.append(physicalPlan)

        return physicalPlans

    def createPhysicalPlanCandidates(
        self,
        max: int = None,
        min: int = None,
        sentinels: bool = False,
        sample_execution_data: List[Dict[str, Any]] = None,
        allow_model_selection: bool = False,
        allow_codegen: bool = False,
        allow_token_reduction: bool = False,
        pareto_optimal: bool = True,
        include_baselines: bool = False,
        shouldProfile: bool = False,
    ) -> List[PhysicalPlan]:
        """Return a set of physical trees of operators."""
        # only fetch sentinel plans if specified
        if sentinels:
            models = self._getModels()
            assert (
                len(models) > 0
            ), "No models available to create physical plans! You must set at least one of the following environment variables: [OPENAI_API_KEY, TOGETHER_API_KEY, GOOGLE_API_KEY]"
            sentinel_plans = [self._createSentinelPlan(model) for model in models]
            return sentinel_plans

        # create set of logical plans (e.g. consider different filter/join orderings)
        logicalPlans = LogicalOperator._createLogicalPlans(self)
        print(f"LOGICAL PLANS: {len(logicalPlans)}")

        # iterate through logical plans and evaluate multiple physical plans
        physicalPlans = [
            physicalPlan
            for logicalPlan in logicalPlans
            for physicalPlan in logicalPlan._createPhysicalPlans(
                allow_model_selection=allow_model_selection,
                allow_codegen=allow_codegen,
                allow_token_reduction=allow_token_reduction,
                shouldProfile=shouldProfile,
            )
        ]
        print(f"INITIAL PLANS: {len(physicalPlans)}")

        # compute estimates for every operator
        op_filters_to_estimates = {}
        if sample_execution_data is not None and sample_execution_data != []:
            # construct full dataset of samples
            df = pd.DataFrame(sample_execution_data)

            # get unique set of operator filters:
            # - for base/cache scans this is very simple
            # - for filters, this is based on the unique filter string or function (per-model)
            # - for induce, this is based on the generated field(s) (per-model)
            op_filters_to_estimates = {}
            logical_op = logicalPlans[0]
            while logical_op is not None:
                op_filter, estimates = None, None
                if isinstance(logical_op, BaseScan):
                    op_filter = "op_name == 'base_scan'"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, CacheScan):
                    op_filter = "op_name == 'cache_scan'"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, ConvertScan):
                    generated_fields_str = "-".join(sorted(logical_op.generated_fields))
                    op_filter = f"(generated_fields == '{generated_fields_str}') & (op_name == 'induce' | op_name == 'p_induce')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        # compute estimates per-model, and add None which forces computation of avg. across all models
                        models = self._getModels(include_vision=True) + [None]
                        estimates = {model: None for model in models}
                        for model in models:
                            model_name = model.value if model is not None else None
                            est_tokens = StatsProcessor._est_num_input_output_tokens(
                                op_df, model_name=model_name
                            )
                            model_estimates = {
                                "time_per_record": StatsProcessor._est_time_per_record(
                                    op_df, model_name=model_name
                                ),
                                "cost_per_record": StatsProcessor._est_usd_per_record(
                                    op_df, model_name=model_name
                                ),
                                "est_num_input_tokens": est_tokens[0],
                                "est_num_output_tokens": est_tokens[1],
                                "selectivity": StatsProcessor._est_selectivity(
                                    df, op_df, model_name=model_name
                                ),
                                "quality": StatsProcessor._est_quality(
                                    op_df, model_name=model_name
                                ),
                            }
                            estimates[model_name] = model_estimates
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, FilteredScan):
                    filter_str = (
                        logical_op.filter.filterCondition
                        if logical_op.filter.filterCondition is not None
                        else str(logical_op.filter.filterFn)
                    )
                    op_filter = f"(filter == '{str(filter_str)}') & (op_name == 'filter' | op_name == 'p_filter')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        models = (
                            self._getModels()
                            if logical_op.filter.filterCondition is not None
                            else [None]
                        )
                        estimates = {model: None for model in models}
                        for model in models:
                            model_name = model.value if model is not None else None
                            est_tokens = StatsProcessor._est_num_input_output_tokens(
                                op_df, model_name=model_name
                            )
                            model_estimates = {
                                "time_per_record": StatsProcessor._est_time_per_record(
                                    op_df, model_name=model_name
                                ),
                                "cost_per_record": StatsProcessor._est_usd_per_record(
                                    op_df, model_name=model_name
                                ),
                                "est_num_input_tokens": est_tokens[0],
                                "est_num_output_tokens": est_tokens[1],
                                "selectivity": StatsProcessor._est_selectivity(
                                    df, op_df, model_name=model_name
                                ),
                                "quality": StatsProcessor._est_quality(
                                    op_df, model_name=model_name
                                ),
                            }
                            estimates[model_name] = model_estimates
                    op_filters_to_estimates[op_filter] = estimates

                elif isinstance(logical_op, LimitScan):
                    op_filter = "(op_name == 'limit')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif (
                    isinstance(logical_op, ApplyAggregateFunction)
                    and logical_op.aggregationFunction.funcDesc == "COUNT"
                ):
                    op_filter = "(op_name == 'count')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                elif (
                    isinstance(logical_op, ApplyAggregateFunction)
                    and logical_op.aggregationFunction.funcDesc == "AVERAGE"
                ):
                    op_filter = "(op_name == 'average')"
                    op_df = df.query(op_filter)
                    if not op_df.empty:
                        estimates = {
                            "time_per_record": StatsProcessor._est_time_per_record(
                                op_df
                            )
                        }
                    op_filters_to_estimates[op_filter] = estimates

                logical_op = logical_op.inputOp

        # estimate the cost (in terms of USD, latency, throughput, etc.) for each plan
        plans = []
        cost_est_data = (
            None if op_filters_to_estimates == {} else op_filters_to_estimates
        )
        for physicalPlan in physicalPlans:
            planCost, fullPlanCostEst = physicalPlan.estimateCost(
                cost_est_data=cost_est_data
            )

            totalTime = planCost["totalTime"]
            totalCost = planCost["totalUSD"]  # for now, cost == USD
            quality = planCost["quality"]

            plans.append((totalTime, totalCost, quality, physicalPlan, fullPlanCostEst))

        # drop duplicate plans in terms of time, cost, and quality, as these can cause
        # plans on the pareto frontier to be dropped if they are "dominated" by a duplicate
        dedup_plans, dedup_desc_set = [], set()
        for plan in plans:
            planDesc = (plan[0], plan[1], plan[2])
            if planDesc not in dedup_desc_set:
                dedup_desc_set.add(planDesc)
                dedup_plans.append(plan)

        print(f"DEDUP PLANS: {len(dedup_plans)}")

        # return de-duplicated set of plans if we don't want to compute the pareto frontier
        if not pareto_optimal:
            if max is not None:
                dedup_plans = dedup_plans[:max]
                print(f"LIMIT DEDUP PLANS: {len(dedup_plans)}")

            return dedup_plans

        # compute the pareto frontier of candidate physical plans and return the list of such plans
        # - brute force: O(d*n^2);
        #   - for every tuple, check if it is dominated by any other tuple;
        #   - if it is, throw it out; otherwise, add it to pareto frontier
        #
        # more efficient algo.'s exist, but they are non-trivial to implement, so for now I'm using
        # brute force; it may ultimately be best to compute a cheap approx. of the pareto front:
        # - e.g.: https://link.springer.com/chapter/10.1007/978-3-642-12002-2_6
        paretoFrontierPlans, baselinePlans = [], []
        for i, (
            totalTime_i,
            totalCost_i,
            quality_i,
            plan,
            fullPlanCostEst,
        ) in enumerate(dedup_plans):
            paretoFrontier = True

            # ensure that all baseline plans are included if specified
            if include_baselines:
                for baselinePlan in [
                    self._createBaselinePlan(model) for model in self._getModels()
                ]:
                    if baselinePlan == plan:
                        baselinePlans.append(
                            (totalTime_i, totalCost_i, quality_i, plan, fullPlanCostEst)
                        )
                        continue

            # check if any other plan dominates plan i
            for j, (totalTime_j, totalCost_j, quality_j, _, _) in enumerate(
                dedup_plans
            ):
                if i == j:
                    continue

                # if plan i is dominated by plan j, set paretoFrontier = False and break
                if (
                    totalTime_j <= totalTime_i
                    and totalCost_j <= totalCost_i
                    and quality_j >= quality_i
                ):
                    paretoFrontier = False
                    break

            # add plan i to pareto frontier if it's not dominated
            if paretoFrontier:
                paretoFrontierPlans.append(
                    (totalTime_i, totalCost_i, quality_i, plan, fullPlanCostEst)
                )

        print(f"PARETO PLANS: {len(paretoFrontierPlans)}")
        print(f"BASELINE PLANS: {len(baselinePlans)}")

        # if specified, grab up to `min` total plans, and choose the remaining plans
        # based on their smallest agg. distance to the pareto frontier; distance is computed
        # by summing the pct. difference to the pareto frontier across each dimension
        def is_in_final_plans(plan, finalPlans):
            # determine if this plan is already in the final set of plans
            for _, _, _, finalPlan, _ in finalPlans:
                if plan == finalPlan:
                    return True
            return False

        finalPlans = paretoFrontierPlans
        for planInfo in baselinePlans:
            if is_in_final_plans(planInfo[3], finalPlans):
                continue
            else:
                finalPlans.append(planInfo)

        if min is not None and len(finalPlans) < min:
            min_distances = []
            for i, (totalTime, totalCost, quality, plan, fullPlanCostEst) in enumerate(
                dedup_plans
            ):
                # determine if this plan is already in the final set of plans
                if is_in_final_plans(plan, finalPlans):
                    continue

                # otherwise compute min distance to plans on pareto frontier
                min_dist, min_dist_idx = np.inf, -1
                for paretoTime, paretoCost, paretoQuality, _, _ in paretoFrontierPlans:
                    time_dist = (totalTime - paretoTime) / paretoTime
                    cost_dist = (totalCost - paretoCost) / paretoCost
                    quality_dist = (
                        (paretoQuality - quality) / quality if quality > 0 else 10.0
                    )
                    dist = time_dist + cost_dist + quality_dist
                    if dist < min_dist:
                        min_dist = dist
                        min_dist_idx = i

                min_distances.append((min_dist, min_dist_idx))

            # sort based on distance
            min_distances = sorted(min_distances, key=lambda tup: tup[0])

            # add closest plans to finalPlans
            k = min - len(finalPlans)
            k_indices = list(map(lambda tup: tup[1], min_distances[:k]))
            for idx in k_indices:
                finalPlans.append(dedup_plans[idx])

        return finalPlans

    def generate_plans(
        self, logical_plan: LogicalPlan, sentinels: bool = False
    ) -> List[PhysicalPlan]:
        """Return a set of possible physical plans."""
        # only fetch sentinel plans if specified
        if sentinels:
            models = self._getModels()
            assert (
                len(models) > 0
            ), "No models available to create physical plans! You must set at least one of the following environment variables: [OPENAI_API_KEY, TOGETHER_API_KEY, GOOGLE_API_KEY]"
            sentinel_plans = [self._createSentinelPlan(logical_plan, model) for model in models] # TODO
            return sentinel_plans

        # compute all physical plans for this logical plan
        physicalPlans = [
            physicalPlan for physicalPlan in self._createPhysicalPlans(logical_plan)
        ]
        print(f"INITIAL PLANS: {len(physicalPlans)}")

        

        # drop duplicate plans in terms of time, cost, and quality, as these can cause
        # plans on the pareto frontier to be dropped if they are "dominated" by a duplicate
        dedup_plans, dedup_desc_set = [], set()
        for plan in plans:
            planDesc = (plan[0], plan[1], plan[2])
            if planDesc not in dedup_desc_set:
                dedup_desc_set.add(planDesc)
                dedup_plans.append(plan)

        print(f"DEDUP PLANS: {len(dedup_plans)}")

        # return de-duplicated set of plans if we don't want to compute the pareto frontier
        if not pareto_optimal:
            if max is not None:
                dedup_plans = dedup_plans[:max]
                print(f"LIMIT DEDUP PLANS: {len(dedup_plans)}")

            return dedup_plans

        # compute the pareto frontier of candidate physical plans and return the list of such plans
        # - brute force: O(d*n^2);
        #   - for every tuple, check if it is dominated by any other tuple;
        #   - if it is, throw it out; otherwise, add it to pareto frontier
        #
        # more efficient algo.'s exist, but they are non-trivial to implement, so for now I'm using
        # brute force; it may ultimately be best to compute a cheap approx. of the pareto front:
        # - e.g.: https://link.springer.com/chapter/10.1007/978-3-642-12002-2_6
        paretoFrontierPlans, baselinePlans = [], []
        for i, (
            totalTime_i,
            totalCost_i,
            quality_i,
            plan,
            fullPlanCostEst,
        ) in enumerate(dedup_plans):
            paretoFrontier = True

            # ensure that all baseline plans are included if specified
            if include_baselines:
                for baselinePlan in [
                    self._createBaselinePlan(model) for model in self._getModels()
                ]:
                    if baselinePlan == plan:
                        baselinePlans.append(
                            (totalTime_i, totalCost_i, quality_i, plan, fullPlanCostEst)
                        )
                        continue

            # check if any other plan dominates plan i
            for j, (totalTime_j, totalCost_j, quality_j, _, _) in enumerate(
                dedup_plans
            ):
                if i == j:
                    continue

                # if plan i is dominated by plan j, set paretoFrontier = False and break
                if (
                    totalTime_j <= totalTime_i
                    and totalCost_j <= totalCost_i
                    and quality_j >= quality_i
                ):
                    paretoFrontier = False
                    break

            # add plan i to pareto frontier if it's not dominated
            if paretoFrontier:
                paretoFrontierPlans.append(
                    (totalTime_i, totalCost_i, quality_i, plan, fullPlanCostEst)
                )

        print(f"PARETO PLANS: {len(paretoFrontierPlans)}")
        print(f"BASELINE PLANS: {len(baselinePlans)}")

        # if specified, grab up to `min` total plans, and choose the remaining plans
        # based on their smallest agg. distance to the pareto frontier; distance is computed
        # by summing the pct. difference to the pareto frontier across each dimension
        def is_in_final_plans(plan, finalPlans):
            # determine if this plan is already in the final set of plans
            for _, _, _, finalPlan, _ in finalPlans:
                if plan == finalPlan:
                    return True
            return False

        finalPlans = paretoFrontierPlans
        for planInfo in baselinePlans:
            if is_in_final_plans(planInfo[3], finalPlans):
                continue
            else:
                finalPlans.append(planInfo)

        if min is not None and len(finalPlans) < min:
            min_distances = []
            for i, (totalTime, totalCost, quality, plan, fullPlanCostEst) in enumerate(
                dedup_plans
            ):
                # determine if this plan is already in the final set of plans
                if is_in_final_plans(plan, finalPlans):
                    continue

                # otherwise compute min distance to plans on pareto frontier
                min_dist, min_dist_idx = np.inf, -1
                for paretoTime, paretoCost, paretoQuality, _, _ in paretoFrontierPlans:
                    time_dist = (totalTime - paretoTime) / paretoTime
                    cost_dist = (totalCost - paretoCost) / paretoCost
                    quality_dist = (
                        (paretoQuality - quality) / quality if quality > 0 else 10.0
                    )
                    dist = time_dist + cost_dist + quality_dist
                    if dist < min_dist:
                        min_dist = dist
                        min_dist_idx = i

                min_distances.append((min_dist, min_dist_idx))

            # sort based on distance
            min_distances = sorted(min_distances, key=lambda tup: tup[0])

            # add closest plans to finalPlans
            k = min - len(finalPlans)
            k_indices = list(map(lambda tup: tup[1], min_distances[:k]))
            for idx in k_indices:
                finalPlans.append(dedup_plans[idx])

        return finalPlans

    def generate_plans(self, logical_plan: LogicalPlan) -> List[PhysicalPlan]:
        """Return a set of possible physical plans."""

        # Stub of physical planning code
        for logical_op in logical_plan:
            applicable_ops = [
                phy
                for phy in self.physical_ops
                if phy.inputSchema == logical_op.inputSchema
                and phy.outputSchema == logical_op.outputSchema
            ]  # Here this should be double checked
